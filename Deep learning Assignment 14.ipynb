{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c4a9e8d",
   "metadata": {},
   "source": [
    "# 1. Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f63086",
   "metadata": {},
   "source": [
    "The dataset has four different classes of two different features. The network has one hidden and input layer with two neurons, an output layer with four neurons. ... Hence to break this symmetry the weights connected to the same neuron should not be initialized to the same value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fd7459",
   "metadata": {},
   "source": [
    "# 2. Is it okay to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc05b6e6",
   "metadata": {},
   "source": [
    "It is possible and common to initialize the biases to be zero, since the asymmetry breaking is provided by the small random numbers in the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2911f37",
   "metadata": {},
   "source": [
    "# 3. Name three advantages of the ELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce35312",
   "metadata": {},
   "source": [
    "ELU becomes smooth slowly until its output equal to -Î± whereas RELU sharply smoothes. ELU is a strong alternative to ReLU. Unlike to ReLU, ELU can produce negative outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a952b56",
   "metadata": {},
   "source": [
    "# 4. In which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b84d39",
   "metadata": {},
   "source": [
    "Sigmoid and tanh should be avoided due to vanishing gradient problem.\n",
    "Softplus and Softsign should also be avoided as Relu is a better choice.\n",
    "Relu should be preferred for hidden layers. ...\n",
    "For deep networks, swish performs better than relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760e921f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
