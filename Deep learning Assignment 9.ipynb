{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8dd81dc",
   "metadata": {},
   "source": [
    "# 1. What are the main tasks that autoencoders are used for?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a205fea",
   "metadata": {},
   "source": [
    "An autoencoder is an unsupervised learning technique for neural networks that learns efficient data representations (encoding) by training the network to ignore signal “noise.” Autoencoders can be used for image denoising, image compression, and, in some cases, even generation of image data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cbaacc",
   "metadata": {},
   "source": [
    "# 2. Suppose you want to train a classifier, and you have plenty of unlabeled training data but only a few thousand labeled instances. How can autoencoders help? How would you proceed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff61cb6b",
   "metadata": {},
   "source": [
    "This is usually the preferred approach when you have a small amount of labeled data and a large amount of unlabeled data. ... Hence, it is sometimes referred to as self-supervised learning but these terms have been used interchangeably in literature to refer to the same approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0e2e22",
   "metadata": {},
   "source": [
    "# 3. If an autoencoder perfectly reconstructs the inputs, is it necessarily a good autoencoder? How can you evaluate the performance of an autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058d13c8",
   "metadata": {},
   "source": [
    "Autoencoder can improve learning accuracy with regularization, which can be a sparsity regularizer, either a contractive regularizer [5], or a denoising form of regularization [6]. Recent work [7] has shown that regularization can be used to prevent feature co-adaptation by dropout training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51ad3a8",
   "metadata": {},
   "source": [
    "# 4. What are undercomplete and overcomplete autoencoders? What is the main risk of an excessively undercomplete autoencoder? What about the main risk of an overcomplete autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab6be66",
   "metadata": {},
   "source": [
    "The only difference between the two is in the encoding output's size. In the diagram above, this refers to the encoding output's size after our first affine function (yellow box) and non-linear function (pink box)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102d34b1",
   "metadata": {},
   "source": [
    "# 5. How do you tie weights in a stacked autoencoder? What is the point of doing so?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc934bb",
   "metadata": {},
   "source": [
    "An autoencoder with tied weights has decoder weights that are the transpose of the encoder weights; this is a form of parameter sharing, which reduces the number of parameters of the model. ... It is therefore a common practice to tie weights when building a symmetrical autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542b0da6",
   "metadata": {},
   "source": [
    "# 6. What is a generative model? Can you name a type of generative autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38753b2e",
   "metadata": {},
   "source": [
    "A generative model includes the distribution of the data itself, and tells you how likely a given example is. For example, models that predict the next word in a sequence are typically generative models (usually much simpler than GANs) because they can assign a probability to a sequence of words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
