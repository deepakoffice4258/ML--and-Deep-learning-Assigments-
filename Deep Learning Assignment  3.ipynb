{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf9f3fb",
   "metadata": {},
   "source": [
    "# 1. Is it OK to initialize all the weights to the same value as long as that value is selectedm randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013fb913",
   "metadata": {},
   "source": [
    "Now imagine that you initialize all weights to the same value (e.g. zero or one). In this case, each hidden unit will get exactly the same signal. E.g. if all weights are initialized to 1, each unit gets signal equal to sum of inputs (and outputs sigmoid(sum(inputs)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec986a7",
   "metadata": {},
   "source": [
    "# 2. Is it OK to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d042009c",
   "metadata": {},
   "source": [
    "It is possible and common to initialize the biases to be zero, since the asymmetry breaking is provided by the small random numbers in the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f91add",
   "metadata": {},
   "source": [
    "# 3. Name three advantages of the SELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7be699f",
   "metadata": {},
   "source": [
    "RELU is clearly converging much faster than SELU . ... Still, RELU seems to be doing a much better job than SELU for the default configuration. This behavior remains more or less the same after iterating through hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e648b989",
   "metadata": {},
   "source": [
    "# 4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6517e4",
   "metadata": {},
   "source": [
    "Selu:-The rectified linear activation function overcomes the vanishing gradient problem, allowing models to learn faster and perform better.\n",
    "Leaky ReLU:-Leaky ReLUs are one attempt to fix the “dying ReLU” problem. Instead of the function being zero when x < 0, a leaky ReLU will instead have a small positive slope (of 0.01, or so).\n",
    "tanh:-t is very similar to the sigmoid activation function and even has the same S-shape. The function takes any real value as input and outputs values in the range -1 to 1\n",
    "Softmax:-The softmax function is used as the activation function in the output layer of neural network models that predict a multinomial probability distribution. That is, softmax is used as the activation function for multi-class classification problems where class membership is required on more than two class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8b79ea",
   "metadata": {},
   "source": [
    "# 5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f69970",
   "metadata": {},
   "source": [
    "A learning rate that is too large can cause the model to converge too quickly to a suboptimal solution, whereas a learning rate that is too small can cause the process to get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9ff09f",
   "metadata": {},
   "source": [
    "# 6. Name three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb9e4d0",
   "metadata": {},
   "source": [
    "The solution to representing and working with sparse matrices is to use an alternate data structure to represent the sparse data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576295d2",
   "metadata": {},
   "source": [
    "# 7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422154e6",
   "metadata": {},
   "source": [
    "In the original implementation of dropout, dropout does work in both training time and inference time. During training time, dropout randomly sets node values to zero. ... So dropout randomly kills node values with “dropout probability” 1 − p keep .MC-dropout is a method of performing multiple stochastic forward passes with the means of activated dropout in a neural network during the testing process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
