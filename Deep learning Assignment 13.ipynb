{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d33d801",
   "metadata": {},
   "source": [
    "# 1. Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron (i.e., a single layer of linear threshold units trained using the Perceptron training algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ff5711",
   "metadata": {},
   "source": [
    "Originally a perceptron was only referring to neural networks with a step function as the transfer function. In that case of course the difference is that the logistic regression uses a logistic function and the perceptron uses a step function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8305206",
   "metadata": {},
   "source": [
    "# 2. Why was the logistic activation function a key ingredient in training the first MLPs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daef60a",
   "metadata": {},
   "source": [
    "Because the derivative of the logistic function is always nonzero, so Gradient Descent can always roll down the slope. When the activation function is a step function, Gradient Descent cannot move, as there is no slope at all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c195a33f",
   "metadata": {},
   "source": [
    "# 3. Name three popular activation functions. Can you draw them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f60126",
   "metadata": {},
   "source": [
    "The rectified linear activation function, or ReLU activation function, is perhaps the most common function used for hidden layers. It is common because it is both simple to implement and effective at overcoming the limitations of other previously popular activation functions, such as Sigmoid and Tanh."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6d6a84",
   "metadata": {},
   "source": [
    "# 4. What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d09c5fd",
   "metadata": {},
   "source": [
    "In contrast, reverse-mode auto diff is simply a technique used to compute gradients efficiently and it happens to be used by backpropagation. This is a sort of recursive definition: backpropagation consists of multiple backpropagation steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2fe746",
   "metadata": {},
   "source": [
    "# 5. Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de5018c",
   "metadata": {},
   "source": [
    "Split the data at hand into training and test subsets.\n",
    "Repeat optimization loop a fixed number of times or until a condition is met: ...\n",
    "Compare all metric values and choose the hyperparameter set that yields the best metric value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
